## 海量大数据处理面试题和思路总结

所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。那解决办法呢？

针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/数据库或倒排索引/trie树；

针对空间，无非就一个办法：大而化小，分而治之（hash映射），你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛；

至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互)，而集群，机器有多辆，适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。

### 1.常见思路

分而治之/hash映射 + hash统计 + 堆/快速/归并排序

1. 分而治之/hash映射：针对数据太大，内存受限，只能把大文件化成(取模映射)小文件
1. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(key，value)来进行频率统计
1. 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的key

多层划分

多层划分—-其实本质上还是分而治之的思想，重在"分"的技巧上！

适用范围：第k大，中位数，不重复或重复的数字。

基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。

### 2.常见数据结构

#### 1.布隆过滤器 Bloom filter

适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集。

基本原理：当一个元素被加入集合时，通过K个Hash函数将这个元素映射成一个位阵列（Bit array）中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检索元素一定不在；如果都是1，则被检索元素很可能在。

Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。

#### 2.位图 Bitmap

Bit-map就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来表示某个元素是否存在，因此在存储空间方面，可以大大节省。

Bitmap排序方法：

第一步，将所有的位都置为0，从而将集合初始化为空。

第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。

第三步，检验每一位，如果该位为1，就输出对应的整数。

Bloom filter可以看做是对bit-map的扩展。

#### 3.Trie树

适用范围：数据量大，重复多，但是数据种类小可以放入内存。

基本原理及要点：实现方式，节点孩子的表示方式。

扩展：压缩实现。

#### 4.数据库索引

适用范围：大数据量的增删改查。

基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。

#### 5.倒排索引

适用范围：搜索引擎，关键字查询。

基本原理及要点：一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。

4.外排序

适用范围：大数据的排序，去重。

基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树

#### 6.分布式处理之Hadoop/Mapreduce

MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。

适用范围：数据量大，但是数据种类小可以放入内存。

基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。

### 3.**海量数据处理题目**

#### 1.海量日志数据，提取出某日访问百度次数最多的那个IP

1. 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址
1. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计
1. 堆/快速排序：统计完了之后，可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP

> Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。

#### 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询

原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

数据规模大，一次处理不了，我们就需要将数据通过hash映射切分；而本题的情况属于数据量可以一次放入内存(300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理)，所以只是需要一个合适的数据结构。

所以我们在此直接读数据进行hash统计，统计后的数据只有0.75G，可以直接进行排序，而对这种TopK问题，一般是采用堆来解决。

1. hash_map统计：先对这批海量数据预处理
   具体方法：维护一个Key为Query字串，Value为该Query出现次数的HashMap，即hashmap(Query，Value)，每次读取一个Query，如果该字串不在HashMap中，那么加入该字串，并且将Value值设为1；如果该字串在HashMap中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计。
1. 堆排序：借助堆这个数据结构，找出TopK，时间复杂度为O(NlogK)。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N’ * O（logK），（N为1000万，N’为300万）。

方案2：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。

#### 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，返回频数最高的100个词

1. 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
1. hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
1. 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

#### 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的Top10

如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数Top10的数据元素：求出每台电脑上的Top10后，然后把这100台电脑上的Top10组合起来，共1000个数据，再利用上面类似的方法求出Top10就可以了。

但如果同一个元素重复出现在不同的电脑中，则有两种方法：

- 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出Top10，继而组合100台电脑上的Top10，找出最终的Top10。
- 暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出Top10。

#### 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序

方案1：

1. hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
1. hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query,query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。
1. 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）

方案2：

一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

方案3：

与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。

#### 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url

可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

方案1：

1. 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
1. hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

方案2：

如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

#### 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数

方案1：分而治之/hash映射，然后hashmap统计，最后找出所有value为1的key值。

方案2：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。

#### 8.怎么在海量数据中找出重复次数最多的一个

1. 先做hash，然后求模映射为小文件
1. 通过hashmap求出每个小文件中重复次数最多的一个，并记录重复次数
1. 然后比较所有小文件中出现最多的数，最大的就是重复次数最多

#### 9.上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据

上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map来进行统计次数。然后利用堆取出前N个出现次数最多的数据。

#### 10.一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析

方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。

方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是`O(n*le)`（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是`O(nlog10)`。

#### 11.1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现

方案1：这题用trie树比较合适，hash_map也行。

方案2：分而治之/hash映射 + hashmap

#### 12.一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解

1. 首先根据用hash并求模，将文件分解为多个小文件
1. 对于单个文件利用hashmap求出每个文件件中10个最常出现的词
1. 然后再进行归并处理，找出最终的10个最常出现的词

#### 13.腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

方案1：申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；这里我们把40亿个数中的每一个用32位的二进制来表示假设这40亿个数开始放在一个文件中。

然后将这40亿个数分成两类: 最高位为0和最高位为1 并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；与要查找的数的最高位比较并接着进入相应的文件再查找。再然后把这个文件为又分成两类: 次最高位为0和次最高位为1。

并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；与要查找的数的次最高位比较并接着进入相应的文件再查找。以此类推，就可以找到了，而且时间复杂度为O(logn)，方案2完。

附：这里，再简单介绍下，位图方法：使用位图法判断整形数组是否存在重复 判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。

位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效率还能提高一倍。

#### 14.10亿个QQ号，让我找出一个QQ号是不是在其中，时间复杂度要求O(1)

用bitmap来做这个问题。首先对数据进行预处理。定义10亿bit位个int.在32位计算机下，一个int是32位，10亿位的话，就需要10亿除以32个int整数。大概有很多个。第一个int标记0-31这个数字范围的QQ号存不存在，比如说0000001这个QQ号，我就把第一个int的第1位置1。第二个int能够标记32-63这个范围的QQ存不存在，以此类推。把这10亿个QQ号预处理一遍。然后计算你给我的这个QQ号，它是在哪个int里面，然后找到相应的数据位，看是1还是0，就能在O(1)的时间里找到。

#### 15.移动公司需要对已经发放的所有139段的号码进行统计排序，已经发放的139号码段的文件都存放在一个文本文件中（原题是放在两个文件中），一个号码一行，现在需要将文件里的所有号码进行排序，并写入到一个新的文件中；号码可能会有很多，最多可能有一亿个不同的号码（所有的139段号码），存入文本文件中大概要占1.2G的空间；jvm最大的内存在300以内，程序要考虑程序的可执行性及效率；只能使用Java标准库，不得使用第三方工具。

方案1：

1. 顺序读取存放号码文件的中所有号码，并取139之后的八位转换为int类型；每读取号码数满一百万个（这个数据可配置）将已经读取的号码排序并存入新建的临时文件
1. 将所有生成的号码有序的临时文件合并存入结果文件

> 这个算法虽然解决了空间问题，但是运行效率极低，由于IO读写操作太多，加上步骤1中的排序的算法（快速排序）本来效率就不高


方案2：bitmap

一个号码占一个bit，一共需要99999999bit，一个int32位，所以需要312.5万个int值，即1250万Byte = 12.5M，算法如下：

1. 初始化bits[capacity]
1. 顺序所有读入电话号码，并转换为int类型，修改位向量值bits[phoneNum]=1
1. 遍历bits数组，如果bits[index]=1，转换index为电话号码输出

#### 16.100万个数中找出最大的100个数

方案1：在前面的题中，我们已经提到，用一个含100个元素的最小堆完成。复杂度为O(100万*lg100)。

方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，直到比轴大的一部分比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100万*100)。

方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素移除，并把x利用插入排序的思想，插入到序列L中。依次循环，直到扫描了所有元素，复杂度为O(100万*100)。